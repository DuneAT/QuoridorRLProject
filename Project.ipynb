{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Quoridor_agent import *\n",
    "from Quoridor_environment import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_REWARD = 100  # The reward gained by the winner of game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(render, n=5):\n",
    "    '''\n",
    "    Returns a new AI Gym environment for the Quoridor game\n",
    "\n",
    "        Parameters:\n",
    "            render (string): \"human\" to display the game\n",
    "                             \"rgb_array\" otherwise\n",
    "            n (int): the size of the board\n",
    "\n",
    "        Returns:\n",
    "            env: the AI Gym environment\n",
    "            state_dim (int): the number of possible states\n",
    "            action_dim (int): the number of possible actions\n",
    "    '''\n",
    "    env = QuoridorWorld(render_mode=render, grid_size=n, n_fences=3)\n",
    "    state_dim = 2*n*n + 2\n",
    "    action_dim = 4+2*n*n\n",
    "    return env, state_dim, action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation2state(obs, state_dim, n):\n",
    "    '''\n",
    "    Returns the environment state in the proper form\n",
    "\n",
    "        Parameters:\n",
    "            obs: the output of the environment step function\n",
    "            state_dim (int): the number of possible states of the environment\n",
    "            n (int): the size of the board\n",
    "    '''\n",
    "    a = np.zeros((1, state_dim))\n",
    "    k = ij2k(obs[\"player_1\"][0], obs[\"player_1\"][1], n)\n",
    "    a[0,k] = 1\n",
    "    k = ij2k(obs[\"player_2\"][0], obs[\"player_2\"][1], n)\n",
    "    a[0,n*n+k] = 1\n",
    "    a[0,-2] = obs[\"fences_player_1\"]\n",
    "    a[0,-1] = obs[\"fences_player_2\"]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_agent_V0(env):\n",
    "    \"\"\"\n",
    "    Fully-random dummy agent. It acts and returns a bool to indicate if it won\n",
    "\n",
    "        Parameters:\n",
    "            env: the environment\n",
    "    \"\"\"\n",
    "    _actions = [0,1,2,3]\n",
    "    if env.r_fences[1]>0:\n",
    "        f = env.fences\n",
    "        for k, i in enumerate(f):\n",
    "            if i == 0:\n",
    "                _actions.append(4+k)\n",
    "                pass\n",
    "    flag = True\n",
    "    while flag:\n",
    "        if len(_actions) == 0:\n",
    "            break\n",
    "        act = random.choices(_actions)[0]\n",
    "        state_, reward, done, flag, info = env.step([2,act])\n",
    "        _actions.remove(act)\n",
    "    return done\n",
    "\n",
    "def dummy_agent_V1(env):\n",
    "    \"\"\"\n",
    "    Stochastic dummy agent: tends to choose the good direction\n",
    "    \n",
    "        Parameters:\n",
    "            env: the environment\n",
    "    \"\"\"\n",
    "    _actions = [0,1,2,3]\n",
    "    _weights = [4, 8, 50, 8]\n",
    "    if env.r_fences[1]>0:\n",
    "        f = env.fences\n",
    "        for k, i in enumerate(f):\n",
    "            if i == 0:\n",
    "                _actions.append(4+k)\n",
    "                _weights.append(1)\n",
    "                pass\n",
    "    flag = True\n",
    "    while flag:\n",
    "        if len(_actions) == 0:\n",
    "            break\n",
    "        act = random.choices(_actions, weights=_weights)[0]\n",
    "        state_, reward, done, flag, info = env.step([2,act])\n",
    "        l = _actions.index(act)\n",
    "        _actions.remove(act)\n",
    "        _weights.pop(l)\n",
    "    return done\n",
    "\n",
    "def agent_act(agent, env, player, state, state_dim, n):\n",
    "    \"\"\"\n",
    "    The agent plays a step\n",
    "\n",
    "        Parameters:\n",
    "            agent: the playing agent\n",
    "            env: the environment\n",
    "            player (int): 1 if first (= comes from left) player\n",
    "                          2 otherwise\n",
    "            state: current state of the environment\n",
    "            state_dim (int): the number of possible states of the environment\n",
    "            n (int): the size of the board\n",
    "\n",
    "        Returns:\n",
    "            observation: next state of the environment (after the action of the agent)\n",
    "            reward (int): the reward gained by the agent\n",
    "            done (bool): True if the agent won\n",
    "            flag (bool): True if the chosen action is illegal\n",
    "            action (int): the chosen action\n",
    "    \"\"\"\n",
    "    _acts = [0,1,2,3]\n",
    "    if env.r_fences[player-1]>0:\n",
    "        f = env.fences\n",
    "        for k, i in enumerate(f):\n",
    "            if i == 0:\n",
    "                _acts.append(4+k)\n",
    "\n",
    "    flag = True\n",
    "    while flag and len(_acts) != 0:\n",
    "        # Run agent on the state\n",
    "        action = agent.act(state=state, actions = _acts)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, flag, info = env.step([player,action])\n",
    "        _acts.remove(action)\n",
    "    if done:\n",
    "        reward = WIN_REWARD\n",
    "    return (observation2state(next_state, state_dim, n), reward, done, flag, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(agent, name):\n",
    "    \"\"\"\n",
    "    Saves the agent parameters into the 'name' file\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'online_state_dict': agent.net.online.state_dict(),\n",
    "        'target_state_dict': agent.net.target.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    }, name)\n",
    "    return\n",
    "\n",
    "def load_model(agent, name):\n",
    "    \"\"\"\n",
    "    Returns the agent with weights coming from the 'name' file\n",
    "    \"\"\"\n",
    "    cp = torch.load(name)\n",
    "    agent.net.online.load_state_dict(cp['online_state_dict'])\n",
    "    agent.net.target.load_state_dict(cp['target_state_dict'])\n",
    "    agent.optimizer.load_state_dict(cp['optimizer_state_dict'])\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentVSdummy(env, agent, dummy, epochs, learn = True, save = None):\n",
    "    \"\"\"\n",
    "    Plays Quoridor games between an agent (first = left player) and a dummy bot\n",
    "\n",
    "    Parameters:\n",
    "        env: the quoridor environment\n",
    "        agent: the playing agent\n",
    "        dummy: the dummy agent (V0 or V1)\n",
    "        epochs (int): number of played games\n",
    "        learn (bool): if True the agent will be trained\n",
    "        save (string): the name of the file to save the best agent. If None no save will be made\n",
    "\n",
    "    Returns:\n",
    "        agent: the agent\n",
    "        vic_rate: the victory rate of the agent\n",
    "    \"\"\"\n",
    "\n",
    "    cnt_victory = 0\n",
    "    n = env.grid_size\n",
    "    state_dim = 2*n*n + 2\n",
    "    action_dim = 4+2*n*n\n",
    "    logger = MetricLogger(save_dir)\n",
    "    best_reward = float('-inf')\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        s, info = env.reset()\n",
    "        state = observation2state(s, state_dim=state_dim, n=n)\n",
    "        tot_rew = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            next_state, reward, done, flag, action = agent_act(agent, env, 1, state, state_dim=state_dim, n=n)\n",
    "\n",
    "            if not done:\n",
    "                done_dummy = dummy(env)\n",
    "            else: \n",
    "                cnt_victory+=1\n",
    "            \n",
    "            if learn:\n",
    "                if done_dummy:\n",
    "                    reward = -WIN_REWARD\n",
    "                agent.cache(state=state, \n",
    "                          next_state=next_state, \n",
    "                          action=action, \n",
    "                          reward=reward,\n",
    "                          done=done)\n",
    "                q, loss = agent.learn()\n",
    "                logger.log_step(reward=reward, loss=loss, q=q)\n",
    "\n",
    "            state = next_state\n",
    "            tot_rew += reward\n",
    "\n",
    "            if done or done_dummy or flag:\n",
    "                break\n",
    "\n",
    "        if learn:\n",
    "            logger.log_episode()\n",
    "\n",
    "        if learn and _ % 10 == 0:\n",
    "            logger.record(episode=_, epsilon=agent.exploration_rate, step=agent.curr_step)\n",
    "        if save != None:\n",
    "            mean_ep_reward = tot_rew/10\n",
    "            if mean_ep_reward > best_reward:\n",
    "                best_reward = mean_ep_reward\n",
    "                save_model(agent, save)\n",
    "\n",
    "    print(\"Victory rate = \", cnt_victory/epochs)\n",
    "    env.close()\n",
    "    return agent, cnt_victory/epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentVSagent(env, agents, epochs, learn = [True, True]):\n",
    "    \"\"\"\n",
    "    Plays Quoridor games between an agent (first = left player) and a dummy bot\n",
    "\n",
    "    Parameters:\n",
    "        env: the quoridor environment\n",
    "        agents: the playing agents\n",
    "        dummy: the dummy agent (V0 or V1)\n",
    "        epochs (int): number of played games\n",
    "        learn list(bool): if True the agents will be trained\n",
    "\n",
    "    Returns:\n",
    "        agents: the agents\n",
    "        victory_rate: victory rate of the first = left player\n",
    "    \"\"\"\n",
    "\n",
    "    cnt_victory = 0\n",
    "    n = env.grid_size\n",
    "    state_dim = 2*n*n + 2\n",
    "    action_dim = 4+2*n*n\n",
    "    logger0 = MetricLogger(save_dir)\n",
    "    logger1 = MetricLogger(save_dir)    \n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        s, info = env.reset()\n",
    "        state = observation2state(s, state_dim=state_dim, n=n)\n",
    "        turn = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            next_state, reward, done, flag, action = agent_act(agents[0], \n",
    "                                                            env, \n",
    "                                                            1, \n",
    "                                                            state, \n",
    "                                                            state_dim=state_dim, \n",
    "                                                            n=n)\n",
    "\n",
    "            if not done:\n",
    "                final_state, opp_reward, opp_done, opp_flag, opp_action = agent_act(agents[1], \n",
    "                                                                                env, \n",
    "                                                                                2, \n",
    "                                                                                next_state, \n",
    "                                                                                state_dim=state_dim, \n",
    "                                                                                n=n)\n",
    "\n",
    "            else: \n",
    "                cnt_victory+=1\n",
    "                opp_reward = -WIN_REWARD\n",
    "            \n",
    "            if opp_done:\n",
    "                reward = -WIN_REWARD\n",
    "            \n",
    "            if learn[0]:\n",
    "                agents[0].cache(state=state, \n",
    "                                next_state=next_state, \n",
    "                                action=action, \n",
    "                                reward=reward,\n",
    "                                done=done)\n",
    "                q, loss = agents[0].learn()\n",
    "                logger0.log_step(reward=reward, loss=loss, q=q)\n",
    "\n",
    "            if learn[1]:\n",
    "                agents[1].cache(state=next_state, \n",
    "                                next_state=final_state, \n",
    "                                action=opp_action, \n",
    "                                reward=opp_reward,\n",
    "                                done=opp_done)\n",
    "                q, loss = agents[1].learn()\n",
    "                logger1.log_step(reward=opp_reward, loss=loss, q=q)\n",
    "\n",
    "            state = final_state\n",
    "\n",
    "            if done or opp_done or flag:\n",
    "                break\n",
    "        \n",
    "        if learn[0]:\n",
    "            logger0.log_episode()\n",
    "        if learn[1]:\n",
    "            logger1.log_episode()\n",
    "\n",
    "        if e % 10 == 0:\n",
    "            if learn[0]:\n",
    "                logger0.record(episode=e, epsilon=agents[0].exploration_rate, step=agents[0].curr_step)\n",
    "            if learn[1]:\n",
    "                logger1.record(episode=e, epsilon=agents[1].exploration_rate, step=agents[1].curr_step)\n",
    "\n",
    "    env.close()\n",
    "    print(\"Victory rate = \", cnt_victory/epochs)\n",
    "    return agents, cnt_victory/epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 13 - Epsilon 0.9948124617142915 - Mean Reward -60.0 - Mean Length 10.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 5.669 - Time 2023-02-25T00:59:44\n",
      "Victory rate =  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<agent_V1.DQN2 at 0x2817ca0cbb0>, 0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run a simple test to see how it works:\n",
    "\n",
    "test_env, state_dim, action_dim = create_env(\"human\")   # Create the environment\n",
    "test_agent = DQN2(state_dim, action_dim, save_dir)      # Create the agent\n",
    "\n",
    "trained_agent = agentVSdummy(test_env,\n",
    "             test_agent,\n",
    "             dummy_agent_V1,\n",
    "             10,\n",
    "             True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38d7bc3cb9932d03070d5aea1cb4d672945ecb209ad8d628cca8bba29ab1e3fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
